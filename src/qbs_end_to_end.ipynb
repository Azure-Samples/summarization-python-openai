{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Query Based Summarization Solution Playbook\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Knowledge bases in enterprises are very common in the industry today and can have extensive number of documents in different categories. Retrieving relevant content based on a user query is a challenging task.  Given a query we were able to retrieve information accurately at the document level using methods such as Page Rank developed and made highly accurate especially by Google,  after this point the user has to delve into the document and search for the relevant information.  With recent advances in Foundation Transformer Models such as the one developed by Open AI the challenge is alleviated by using “Semantic Search” methods by using encoding information such as “Embeddings” to find the relevant information and then to summarize the content to present to the user in a concise and succinct manner.\n",
        "\n",
        "This Playbook will introduce the releavant use cases and the end-to-end archtitecture for this use case. We wuill take you through the step-by-step process of using various Azure Cognitive Services and more specifically Azure OpenAI's GPT-3 models to perform the downstream task of summarization.\n",
        "\n",
        "This playbook aims to demonstrate how to link various Azure Cognitive Services together to improve results for common downstream tasks in a large enterprise enviorment. The methods and architecture are extremely customizable to your summarization use case and can be applied to many different datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use Cases\n",
        "\n",
        "This playbook links together two main use cases: **Document Search** and **Document Summarization**. By using different Azure Cognitive Services specialized for each task, the end result is focused, succinct, and thoroughly vetted.\n",
        "\n",
        "### Document Search\n",
        "\n",
        "Document Search is driven by an initial user query. The user provides their query to the search service, enabling users to find the top most relevant document from their knowledge base. Document Search leverages the state of the art search technology using indexers. Within large enteprises there are various documents in different formats that are congregated in a large knowledge base. Having the ability to quickly, effectively, and accurately find relevant documents is essential to this end-to-end scenario.\n",
        "\n",
        "### Document Summarization\n",
        "\n",
        "Document Summarization is performed by providing input quality text into the summarizaton engine. The key to successful document summarization is retrieving the most important points within a large document to feed into summarization, as the quality and focus of a summary is only as good as the input data. To ensure the most succinct and focused summary is returned, we utitlize Embeddings to search within a document for the most salient pieces. Document Summarization is essential in large enteprises to condense large documents into human consumable information that frees up humans to do more specialized and techincal work rather than focus on common tasks such as summmarization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Architecture\n",
        "\n",
        "An E2E architecture found in Images/EndToEndArchitecture.png captures the use case that we talked about the previous sections. The user passes their query to a knowledge base to filter the number of documents top most relevant docouments, narrows the scope of documents to tens from a couple of thousands. This cna be done using Azure Cognitive Search by creating a custom index for your knowledge base. Then once a document is selected, the document needs to be segmented and then each segment can be embedded using GPT-3 embedding models. This allows each text chunk to have its semantic meaning captured through an embedding. Then the user can send another query to search within the document and \"zone in\" on a particular text segment or segements based using similarities between the query and embeddings for each text chunk. Once a section is \"zoned-in\", the user can extract the relevant text and pass it to the GPT-3 Completion endpoint for summarization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ensure this notebook is in a virtual environment so you can predictably recreate dependencies using \n",
        "%pip install -r ..\\requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the necessary libraries if you haven't already run pip install -r ..\\requirements.txt above, else run to check if all libraries are installed\n",
        "%pip install python-dotenv\n",
        "%pip install pandas\n",
        "%pip install azure-core\n",
        "%pip install azure-search-documents\n",
        "%pip install openai\n",
        "%pip install matplotlib\n",
        "%pip install plotly\n",
        "%pip install scipy\n",
        "%pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#Get all the connections to Azure with secrets from a local .env file in the root of the project. \n",
        "#Ensure your repo .gitignores the .env file if you are using this approach.\n",
        "load_dotenv()  \n",
        "\n",
        "#Azure Cognitive Search\n",
        "SEARCH_ENDPOINT = os.getenv(\"SEARCH_ENDPOINT\")\n",
        "SEARCH_API_KEY = os.getenv(\"SEARCH_API_KEY\")\n",
        "\n",
        "#Azure OpenAI\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
        "OPENAI_ENDPOINT = os.getenv(\"OPENAI_ENDPOINT\") \n",
        "\n",
        "print(SEARCH_ENDPOINT)\n",
        "print(OPENAI_ENDPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Walk-Through\n",
        "\n",
        "This remainder of this notebook will dive deep into each component of the architecture and demonstrate how to link them together for a successful document summarization use case. \n",
        "\n",
        "Our guiding query that we will use for this playbook is **\"Provide details on the Clinton democratic nomination\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "\n",
        "For this walkthrough, we will be using the **CNN/Daily Mail** dataset. This is a common dataset used for text summarization and question answering tasks. Human generated abstractive summary bullets were generated from news stories in CNN and Daily Mail websites. \n",
        "\n",
        "In all, the corpus has 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs, as defined by their scripts. The source documents in the training set have 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences. We will use a subset of the corpus for this example that can be found in the /data folder. \n",
        "\n",
        "The relevant schema for our work today consists of:\n",
        "\n",
        "+ id: a string containing the heximal formatted SHA1 hash of the URL where the story was retrieved from\n",
        "+ article: a string containing the body of the news article\n",
        "+ highlights: a string containing the highlight of the article as written by the article author\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Knowledge Base: __Azure Blob Storage__\n",
        "\n",
        "Being able to store and access unstructured data at scale is imperative for enterprises. Azure Blob Storage provides storage capabilities with optimized costs with tieired storage for long-term data, and flexibiluty for high perofrmancing computer and ML workloads. The comprehensive data management Azure Blob Storage provides coupled with it's ease of use is why we chose to upload our knowledge base to Azure Blob Storage for this scenario.\n",
        "\n",
        "Around 11.2K articles and their summaries are stored in the Azure cloud for quick aceess. To do this we created a blob services with the dataset uploaded as a jsonl. This blob service is contained wihtin a storage account this will link to other services further in this example.\n",
        "\n",
        "For more information please reference: https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext dotenv\n",
        "\n",
        "from io import StringIO\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "values=!azd env get-values\n",
        "load_dotenv(stream=StringIO(values.nlstr))\n",
        "\n",
        "import os\n",
        "storage_account_name = os.environ.get('AZURE_STORAGE_ACCOUNT_NAME')\n",
        "\n",
        "!az storage blob upload --account-name $storage_account_name --container-name openaiblob --name cnn_dataset.json --file ../data/cnn_dataset.json --au"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enterprise Search: __Azure Cognitive Search__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Azure Cognitive Search is a cloud search service that provides developers infrastructure, APIs, and tools for building a rich search experience. This service creates a search engine for full text search over a search index containing user-owned content. Azure Cognitive Search uses semantic search that brings relevance and language understanding to search results.\n",
        "\n",
        "In this example, we will create a search service within the Azure portal with a customized index that enables us to search the CNN/Daily Mail knowledge base stored in Azure Blob storage.\n",
        "\n",
        "In the image below, we have created an index to search against the 11.2K document stored in the Azure Blob Storage. We use the id, article, and highlights field to be retrievable during the search.\n",
        "\n",
        "[insert image here in MD images\\AzureCogSearchIndex]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our our storage and search services set up through the Azure portal, we can dive into our use case. \n",
        "\n",
        "The user is now able to query against the knowledge base. As a reminder from above, our **guiding initial query is \"Provide details on the Clinton Democratic nomination\"**\n",
        "\n",
        "Lets place that query or a paragraphse of the query into the Azure Cognitive Search service, and see what the top results are.\n",
        "\n",
        "[insert image for AzureCogSearchResults]\n",
        "\n",
        "Lets take a deeper dive into the results below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "\n",
        "index_name = \"azureblob-index\"\n",
        "\n",
        "# Create a client\n",
        "credential = AzureKeyCredential(SEARCH_API_KEY)\n",
        "client = SearchClient(endpoint=SEARCH_ENDPOINT,\n",
        "                      index_name=index_name,\n",
        "                      credential=credential)\n",
        "results = client.search(search_text=\"clinton democratic nomination\")\n",
        "search_df = pd.DataFrame(results)\n",
        "search_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we have selected the top 3 results from our Azure Cognitive Search app. In the results we can see the search score Azure Cognitive Search uses to assess semantic similarity between the query and the knowledge base. \n",
        "\n",
        "At this point in the use case, the user can either select the top article or investigate which of the top results is providing the most relevant information for their use case. By putting ourselves in the customer shoes and reflecting back on our guiding initial query of **clinton democratic nomination**, lets move forward with the first article. \n",
        "\n",
        "The first article highlights Clinton's campaign kick off in the Iowa caucused and focuses on her seeking the democratic nomation. The top search result matched with our initial query!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Zone: __Azure OpenAI Embedding API__\n",
        "\n",
        "Now that we have selected a narrowed on a single document from our knowledge base of 11.2K documents - we can dive deeper into the single document to refine our initial query to a more specific section or \"zone\" of the article.\n",
        "\n",
        "To do this, we will utilize the Azure Open AI Embeddings API. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embeddings Overview\n",
        "\n",
        "An embedding is a special format of data representation that can be\n",
        "easily utilized by machine learning models and algorithms. The embedding\n",
        "is an information dense representation of the semantic meaning of a\n",
        "piece of text. Each embedding is a vector of floating-point numbers,\n",
        "such that the distance between two embeddings in the vector space is\n",
        "correlated with semantic similarity between two inputs in the original\n",
        "format. For example, if two texts are similar, then their vector\n",
        "representations should also be similar.\n",
        "\n",
        "Different Azure OpenAI embedding models are specifically created to be\n",
        "good at a particular task. Similarity embeddings are good at capturing\n",
        "semantic similarity between two or more pieces of text. Text search\n",
        "embeddings help measure long documents are relevant to a short query.\n",
        "Code search embeddings are useful for embedding code snippets and\n",
        "embedding nature language search queries.\n",
        "\n",
        "Embeddings make it easier to do machine learning on large inputs\n",
        "representing words by capturing the semantic similarities in a vector\n",
        "space. Therefore, we can use embeddings to if two text chunks are\n",
        "semantically related or similar, and inherently provide a score to\n",
        "assess similarity.\n",
        "\n",
        "### Cosine Similarity\n",
        "\n",
        "A previously used approach to match similar documents was based on\n",
        "counting maximum number of common words between documents. This is\n",
        "flawed since as the document size increases, the overlap of common words\n",
        "increases even if the topics differ. Therefore cosine similarity is a\n",
        "better approach by using the Euclidean distance.\n",
        "\n",
        "Mathematically, cosine similarity measures the cosine of the angle\n",
        "between two vectors projected in a multi-dimensional space. This is\n",
        "beneficial because if two documents are far apart by Euclidean distance\n",
        "because of size, they could still have a smaller angle between them and\n",
        "therefore higher cosine similarity.\n",
        "\n",
        "The Azure OpenAI embeddings rely on cosine similarity to compute\n",
        "similarity between documents and a query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Segmentation or \"Chunking\"\n",
        "\n",
        "The document that we selected span a few pages. In order to produce a meaningful and a focused summary we must first chunk or segment the document. This is essential for long document summarization for two main reasons:\n",
        "\n",
        "+ Going around the token limitation inherit in a transformer based model - due to the token limitation we cannot pass the entire document into a model \n",
        "+ Creating a mapping from topic to relevant chunks of text - for long documents topics can vary drastically and to produce a meaningful summary, most of the time you want to \"zone\" on a single area. This may be a page or just a section with the information you want to dive into. \n",
        "\n",
        "\n",
        "By chunking the document into logical segments, we can utilize the power of the Azure OpenAI Embeddings following these steps:\n",
        "\n",
        "1. Chunk the document into logical segments that fit within the token limitation\n",
        "2. Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
        "3. Upon receiving a query for the specific document, embed the query in the same vector space as the context chunks from Step 2.\n",
        "4. Find the most relevant context chunks through cosine similarity and use it for your desired downstream tasks. \n",
        "\n",
        "Let's walk through this together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the original text from the top search result from Azure Cognitive Search\n",
        "document=search_df._get_value(0,'article')\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "openai.api_base = OPENAI_ENDPOINT\n",
        "openai.api_version = \"2022-06-01-preview\"\n",
        "\n",
        "TEXT_SEARCH_DOC_EMBEDDING_ENGINE = 'text-search-curie-doc-001'\n",
        "TEXT_SEARCH_QUERY_EMBEDDING_ENGINE = 'text-search-curie-query-001'\n",
        "TEXT_DAVINCI_001 = \"davinci-instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Defining helper functions\n",
        "\n",
        "#Splits text after sentences ending in a period. Combines n sentences per chunk.\n",
        "def splitter(n, s):\n",
        "    pieces = s.split(\". \")\n",
        "    list_out = [\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)]\n",
        "    return list_out\n",
        "\n",
        "# Perform light data cleaning (removing redudant whitespace and cleaning up punctuation)\n",
        "def normalize_text(s, sep_token = \" \\n \"):\n",
        "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
        "    s = re.sub(r\". ,\",\"\",s)\n",
        "    # remove all instances of multiple spaces\n",
        "    s = s.replace(\"..\",\".\")\n",
        "    s = s.replace(\". .\",\".\")\n",
        "    s = s.replace(\"\\n\", \"\")\n",
        "    s = s.strip()\n",
        "    \n",
        "    return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (STEP 1) Chunking the document by creating a chunk every 10th sentence and creating a pandas DF\n",
        "document_chunks = splitter(10, normalize_text(document))\n",
        "df = pd.DataFrame(document_chunks, columns = [\"chunks\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#(STEP 2): Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
        "df['curie_search'] = df[\"chunks\"].apply(lambda x : get_embedding(x, engine = TEXT_SEARCH_DOC_EMBEDDING_ENGINE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a dataframe with the document chunks and their corresponding embedding vectors. We can \"zone in\" more specifically on a certain area of the document by narrowing our query. \n",
        "\n",
        "Lets generate a document specific query that will help us isolate the correct context chunks that we can pass down into further downstream tasks.\n",
        "\n",
        "The document specific query that we will use for Step 3 is **what is the trouble so far in the clinton campaign?**\n",
        "\n",
        "In the code section below, we will pass the document specific query to a function that embeds the query to the same vector space as the document chunks. Calculate the cosine similarity between each context chunk and the query to find the most relevant context chunk based on the input query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (STEP 3) upon receiving a query for the specific document, embed the query in the same vector space as the context chunks\n",
        "def search_docs(df, user_query, top_n=3, to_print=True):\n",
        "    embedding = get_embedding(\n",
        "        user_query,\n",
        "        engine=TEXT_SEARCH_QUERY_EMBEDDING_ENGINE\n",
        "    )\n",
        "    df[\"similarities\"] = df.curie_search.apply(lambda x: cosine_similarity(x, embedding))\n",
        "\n",
        "    res = (\n",
        "        df.sort_values(\"similarities\", ascending=False)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    if to_print:\n",
        "        display(res)\n",
        "    return res\n",
        "\n",
        "document_specific_query = \"trouble so far in clinton campaign\"\n",
        "res = search_docs(df, document_specific_query, top_n=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a deep dive into the results from step 3. They both have very similar cosine simialrity to the input query. Both results talk about current trouble the Clinton campaign is having by staging \"ordinary voter\" and creating fake events in Iowa where the campaign was kick off. \n",
        "\n",
        "The resulting context chunks are relevant to our query and can be used for further downstream tasks. At this point, the user can decide if they want to just take the top result to the downstream task or combine a few context chunks. (Step 4)\n",
        "\n",
        "For this example, let's use both of the chunks as they are both relevant to the Clinton nomination and issues so far during the campaign for our downstream task of summarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_1 = res[\"chunks\"][1] #NOTE: Ensure this matches the index of one of the chunks from step 3\n",
        "print(result_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_2 = res[\"chunks\"][4] #NOTE: Ensure this matches the index of one of the chunks from step 3\n",
        "print(result_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Summarization: __Azure OpenAI Completion API__\n",
        "\n",
        "Now that we have extracted the relevant context chunks using Azure OpenAI Embedding API, we can use these chunks to provide meaningful context for our prompt design. We will be using the **Completion endpoint** for summarization. \n",
        "\n",
        "Together, let's design a prompt used for summarizing our extracted texts.\n",
        "\n",
        "### Prompt Design Refresher\n",
        "\n",
        "GPT-3 models can perform many tasks. Therefore you must be explicit in describing what you want. \n",
        "\n",
        "The models try to guess what you want from the prompt. If you send the words \"Give me a list of cat breeds,\" the model wouldn't automatically assume that you're asking for a list of cat breeds. You could just as easily be asking the model to continue a conversation where the first words are \"Give me a list of cat breeds\" and the next ones are \"and I'll tell you which ones I like.\" If the model only assumed that you wanted a list of cats, it wouldn't be as good at content creation, classification, or other tasks.\n",
        "\n",
        "There are three basic guidelines to creating prompts:\n",
        "\n",
        "**Show and tell**. Make it clear what you want either through instructions, examples, or a combination of the two. If you want the model to rank a list of items in alphabetical order or to classify a paragraph by sentiment, show it that's what you want.\n",
        "\n",
        "**Provide quality data**. If you're trying to build a classifier or get the model to follow a pattern, make sure that there are enough examples. Be sure to proofread your examples — the model is usually smart enough to see through basic spelling mistakes and give you a response, but it also might assume this is intentional and it can affect the response.\n",
        "\n",
        "**Check your settings.** The temperature and top_p settings control how deterministic the model is in generating a response. If you're asking it for a response where there's only one right answer, then you'd want to set these lower. If you're looking for more diverse responses, then you might want to set them higher. The number one mistake people use with these settings is assuming that they're \"cleverness\" or \"creativity\" controls.\n",
        "\n",
        "### Few-Shot Approach\n",
        "\n",
        "The goal of this is to teach the GPT-3 model to learn a conversation style input. We use the “Completion” create OpenAI API and generate a prompt that would best provide us a summary of the conversation. It is important to generate prompts carefully to extract relevant information. To extract general summaries from customer-agent chats, we will be using the following format:\n",
        "\n",
        "1. Prefix: What do you want it to do\n",
        "2. Context primer : Describe what the context is\n",
        "3. Context: # Essentially the information needed to answer the question. In the case of summary, the prose that needs to be summarized. \n",
        "4. Suffix: Describe what form the answer should take. Whether it is an answer, a completion, a summary, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Designing a prompt that will show and tell GPT-3 how to proceed. \n",
        "+ Providing an instruction to summarize the text about the general topic (prefix)\n",
        "+ Providing quality data for the chunks to summarize and specifically mentioning they are the text provided (context + context primer)\n",
        "+ Providing a space for GPT-3 to fill in the summary to follow the format (suffix)\n",
        "'''\n",
        "prompt_i = 'Summarize the content about the Clinton campaign given the text provided.\\n\\Text:\\n'+\" \".join([normalize_text(result_1)])+ '\\n\\nText:\\n'+ \" \".join([normalize_text(result_2)])+'\\n\\nSummary:\\n'\n",
        "\n",
        "prompt_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using a temperature a low temperature to limit the creativity in the response. \n",
        "response = openai.Completion.create(\n",
        "        engine= TEXT_DAVINCI_001,\n",
        "        prompt = prompt_i,\n",
        "        temperature = 0.4,\n",
        "        max_tokens = 500,\n",
        "        top_p = 1.0,\n",
        "        frequency_penalty=0.5,\n",
        "        presence_penalty = 0.5,\n",
        "        best_of = 1\n",
        "    )\n",
        "\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a result, we have a succinct, clear, and impactful summary generated by the Azure OpenAI Completion API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review\n",
        "\n",
        "To summarize, we started by transferring a large knowledge base of data to **Azure Blob Storage**. We also deployed a **Azure Cognitive Search** service by creating a custom indexer for our data. From there, the user provided an initial query in the Azure Cognitive Search app that searched against the Blob Storage. The **enterprise search** results were listed as an output based on Azure Cognitive Search's similarity metric. From there, we **isolated the top search result** and extracated the text. At this point we are now focusing on one document relevant to the initial query. From here, we **chunked** our document to break up the long document into individual chunks. Then we utilized **Azure OpenAI Embedding API** to embed each context vector and asssign a representation semantic meaning to each chunk. Then we created a document specific query that will enable us to **\"zone in\"** on a specific section of the document that we want to use for our downstream task. After passing the document specific query, we used Embeddings to find the most **relevant context chunks** to the query. From there we extracted the relevant context chunks, and designed a **few-shot prompt** for the **Azure OpenAI Completion API** for the downstream task of summarization. \n",
        "\n",
        "From starting from a 11.2K document knowledge base to narrowing onto a specific document, and then finding the relevant inforamtion within the document to generate the best summary - linking Azure Cognitive services is an excellent approach.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.5 ('.venv3': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4267851adf610525093effd93508a73979f2a01ad54843199ac04a86159d9836"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
